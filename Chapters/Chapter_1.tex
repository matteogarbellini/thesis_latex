\newcommand{\wvec}{\begin{pmatrix}1 \\ 0 \end{pmatrix}}
\newcommand{\svec}{{\begin{pmatrix}0 \\ 1 \end{pmatrix}}}
\newcommand{\kalpha}{\ket{\alpha}}
\newcommand{\kbeta}{\ket{\beta}}
\newcommand{\kpsi}{\ket{\psi}}




\newpage
\thispagestyle{empty}
\chapter{\textbf{Preliminaries}}\label{chapter one}
In this chapter we present the basic theoretical knowledge necessary to understand this thesis. We begin by introducing graph theory, random walks and quantum walks. We then discuss the quantum search problem firstly introduced by Grover and the quantum walks implementation for the unstructured search by Childs and Goldstone. Then we present the adiabatic theorem and its application to the adiabatic quantum search, studying the global and local adiabatic evolution. Lastly we look at the differences between the quantum walks approach and the local adiabatic evolution search by Roland and Cerf, which sets the basis for our work.

\section{Introduction to graph theory}\label{sec:introduction to graph theory}
A graph $G$ is defined as a ordered pair $(V,E)$, where $V$ is a set of vertices and $E$ is a set of edges, that represents the connection between any two pair of vertices. A vertex is usually indicated by the coursive letter $j$, and the corresponding edge connecting $j$ to $i$ is given by $(j,i)$. Given a set $V$ of dimension $N$, we refer to a particular vertex with its index, namely $j=1,2,...,N$.\\

\noindent
A graph can be characterized by many properties. Throughout our work we will only consider \textit{simple graphs} characterized by being \textit{undirected}, namely the edges $E$ are symmetric, without self loops such that $(j,j)\notin G$ and having no multiple equivalent edges. Additionally we require the graph to be \textit{connected}, i.e. each vertex can be reached by any other following a path through the available edges.

\noindent
We then define the \textit{vertex degree} $d_j$, that represents, given a vertex $j$, the number of edges that are incident to the vertex $j$\footnote{For an undirected graph the degree does not dependend on the edges incident from or to the selected vertex, while generally this is not true.}. \\

\noindent
If indeed any two vertices $i$ and $j$ are connected by an edge $(i,j)$ we define them as \textit{adjacent}, and from this we can construct an analytical representation of a graph, given by an $N$-dimensional square matrix called \textit{adjacency matrix}, usually referred as $A$. The adjacency matrix is defined as following \footnote{If we had to define the adjacency matrix for an unspecified graph the value of $A_{ij}$ is not necessarily equal to one, but a general $a_{ij}$ since it takes into account the possibilities of self loops and multiple equivalent edges. For the simple graphs consider in this thesis the given definition suffices.}:
\begin{equation}
A_{ij} = \begin{cases} 1 & \mbox{if }(i,j)\in G \\ 0 & \mbox{otherwise} \end{cases}
\end{equation}
which represents the connectivity of the graph. \\
Furthermore, we can introduce a diagonal matrix $D$ that encodes the informations of the vertex degrees for a particular graph $G$. The matrix $D$ is defined as:
\begin{equation}
    D = diag(d_1,..., d_N)
\end{equation}
where $d_j$ is the degree of the vertex $j$.
In this particular context a natural operative basis arises, in which one can associate with each ordered vertex of the graph a vector of the standard basis of the $N$-dimensional vector space. \\ \\


In order to study the dynamics of the system we introduce the \textit{Laplacian matrix L}, also known as the \textit{discrete Laplacian operator}. It is defined as
\begin{equation}
    L = D-A
\end{equation}
where $D$ is the diagonal degrees matrix and $A$ is the adjacenty matrix. The discrete Laplacian operator for the finite, undirected, simple and connected graphs can charactarized by the following properties:
\begin{itemize}
  \item $L$ is symmetric given that both $D$ and $A$ are symmetric
  \item the sum of all elements over a row/column equals to zero
  \item it has a null eigenvalue which corresponds to the eigenvector $\ket{\Psi_0} = \frac{1}{\sqrt{N}}\big(1,1,...,1\big)$
\end{itemize}
As previously mentioned we can associate every vertex with a basis vector of the $N$-dimensional vector space. Similarly we can expand this notation to an $N$-dimensional Hilbert space, where we can use the following Dirac notation to describe the vertex. This formalism also allows us to write the Laplacian matrix in convenient quantum mechanical form using the projectors $\ket{j}\bra{i}$
\begin{equation}
  \ket{1}=\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0\end{bmatrix}, \hspace{15pt}\ket{2}=\begin{bmatrix}0\\1\\\vdots\\0\end{bmatrix}, \hspace{15pt}\ket{N}=\begin{bmatrix}0\\0\\\vdots\\N\end{bmatrix}
\end{equation}


In the following paragraphs we give a brief description of the graphs that will be consider throughout the work. \\
  \vspace{-0.5cm}
    \subsection{Cycle graph}\label{subsec:cycle graph}
    %\addcontentsline{toc}{subsection}{Cycle Graph}
        A $N$-dimensional cycle graph $Cy(N)$ is a monodimensional structure with periodic boundary conditions $\ket{N+1}=\ket{N}$. The Laplacian is given by
        \begin{equation}
            L = 2\sum_{k=1}^{N}\ket{k}\bra{k} - \sum_{k=1}^{N-1}\ket{k}\bra{k+1} - \sum_{k=2}^{N}\ket{k}\bra{k-1}- \ket{N}\bra{1}-\ket{1}\bra{N}
        \end{equation}
        A pictorial representation of a cycle graph is given in \Cref{fig:cyclegraph_pictorial_laplacian} with the corrisponding Laplacian matrix.
        \begin{figure}[htb]
          \centering
          \begin{tabular}{ccc}
            \includegraphics[width=45mm]{./figures/chapter1/cycle} & \hspace{20pt} &\includegraphics[width=40mm]{./figures/chapter1/Laplacian} \\[0.5cm]
          (a)  & & (b) \\[6pt]
          \end{tabular}
          \caption[Pictorial representation of a cycle graph, and matricial representation for the Laplacian]{Pictorial representation of a cycle graph with 5 nodes (a), and the matricial representation for the Laplacian of $Cy(5)$}
          \label{fig:cyclegraph_pictorial_laplacian}
        \end{figure}

    \subsection{Complete graph}\label{subsec:complete graph}
    %\addcontentsline{toc}{subsection}{Complete Graph}
        A graph with $N$ vertices is said to be complete if every node is adjacent to all the other $N-1$ nodes, thus representing a finite bidimensional structure. Its Laplacian matrix is given by:
        \begin{equation}
            L = (N-1)\sum_{j=1}^{N}\ket{j}\bra{j} - \sum_{k\neq j}\ket{j}\bra{k}
        \end{equation}
        A pictorial representation of a complete graph is given in \Cref{fig:complete_graph_pictorial}. We will refer to this type of graph with $C(N)$.
        \begin{figure}[hb]
          \centering
          \includegraphics[width=50mm]{./figures/chapter1/complete}
          \caption[Pictorial representation of a complete graph]{Pictorial representation of a complete graph with $N=7$}
          \label{fig:complete_graph_pictorial}
        \end{figure}

\section{Quantum walks}\label{sec:quantum walks}
The continous-time quantum walk (CTQW) is the direct analogue of the classical continous-time random walk (CTRW). We begin by considering the classical one, expanding later on the quantum mechanical counterpart. \nocite{Mulken2011} \\

A continous-time random walk is a stochastic process that describes the random motion of a particle on a mathematical structure, which in our scenario is a graph $G$.
Let $j$ be a node of a graph $G$, such date the initial state of the system is $\ket{j}$\footnote{With abuse of notation we represent both the classical and the quantum mechanical vertex with the Dirac notation $\ket{j}$}. Then, we denote the transition probability of the walker to go from node $j$ to a node $k$ in a time $t$ with $p_{k,j}(t)$. The state after time $t$ is given by $\ket{j;t}$, such that the overlap with node $k$ is $\braket{k|j;t}=p_{j,k}(t)$. \\
The dynamics that results in the state $\ket{j;t}$ follows from the probability of transitioning from a node to another, also called \textit{transition rates}. In particular these transition rates are the components of the so-called \textit{transfer matrix} $T$, namely $T_{ij}= \bra{k}T\ket{j}$ . If we assume a Markovian process, the following master equation defines the CTRW evolution:
\begin{equation}
  \frac{d}{dt}p_{k,j}(t) = \sum_{l}T_{kl}p_{l,j}(t).
  \label{rw_master}
\end{equation}
In the simplest case, where the transitions rates for all edges are equal, the transfer matrix is closely related to the Laplacian matrix through :
\begin{equation}
  T = -\gamma L
\end{equation}
where $\gamma$ is the transition rate. The solution of \cref{rw_master}, along with the normalization constrains $\sum_{k=1}^{N}p_{k,j}(t) = 1 \hspace{3pt} \forall t$, is given by
\begin{equation}
  p_{ij}(t)= \braket{k|e^{Tt}|j} = \braket{k|e^{-\gamma Lt}|j}.
\end{equation}
Turning to quantum mechanics, the evolution of any physical system obeys the Schroedinger equation, and QWs represent no exception. The dynamics of the CTQW is governed by a specific Hamiltonian $H$, such that the Schroedinger equation for the transition \textit{amplitudes} $\alpha_{i,j}(t)$ is given by
\begin{equation}
  \frac{d}{dt}\alpha_{i,j}(t) = -i \sum_{l}H_{k,l}\alpha_{l,j}(t)
\end{equation}
where $H$ is the Hamiltonian of the system, and for semplicity we assume $\hbar = 1$. The formal solution of such differential equation is given by
\begin{equation}
  \alpha_{l,j}(t) = \braket{k|e^{-iHt}|j}
  \label{qw_master}
\end{equation}
where $e^{-iHt}$ is the quantum mechanical time-evolution operator. We immediately notice the similar structure of equations (\ref{rw_master}) and (\ref{qw_master}), with the only difference - apart from the imaginary unit - that the first is a differential equation for transition probabilities while the latter allows to compute transition amplitudes. The similarity is further pushed by Farhi and Gutmann, proposing to identify the Hamiltonian $H$ of the system with the negative of the classical transfer matrix $T$ \cite{Childs2001}, which as we have seen previously is the Laplacian of the graph \footnote{Please note that in this particular scenario the transition amplitude $\gamma$ is set to one, and assumed to be equal for all vertex transitions.}:
\begin{equation}
  H = -T = L
  \label{qw_Hamiltonian}
\end{equation}
Therefore the Laplacian matrix completely determines the evolution of the quantum system as well as the classical scenario.



\section{Grover's quantum search}\label{sec:grover search}
In 1997 Lov K. Grover addressed the search problem through a quantum mechanical algorithm \cite{Grover1997}. The search problem itself can be formulated in the following way: given an unsorted database containing $N$ items, find one particular item that satisfies a certain condition. Once an item is examined, it is possible to determine whether it represents the solution or not in just one step.
Classically, the most efficient algorithm has to check each and every item in the database individually. If the item checked satisfies the condition the process stops, otherwise it will continue to examine the remaining items until the solution is found. On average, the algorithm will have to check $N/2$ items before finding the desired one.\\On the other hand, Grover's quantum mechanical approach takes advantage of the \textit{superposition} of states in a quantum system, and by having input and output in such superposition it can find the desired item in $O(\sqrt{N})$ \textit{quantum mechanical steps}, instead of $O(N)$ classical steps. As we shall see later, these quantum mechanicals steps consist of rotations due to a unitary operator called \textit{Grover operator}. \\

\noindent
Let us look at the quantum search algorithm more in detail, in particular setting the stage for the search algorithm in terms of an \textit{oracle}, which plays a central role in Grover's search and throughout our work. Additionally, this allows us to present a very general description of the search procedure, and a geometric way to visualize its action \cite{Nielsen2000}. \\

\noindent
Suppose we want to search through a search space of $N$ elements, and let us consider the indices of such elements - instead of the actual elements - that are numbers from 0 to $N-1$. In order to easily store the index in $n$ bits we consider $N=2^n$, and assume that the particular search problem has only one solution. We can now represent one instance of the search, i.e. checking if the item satisfies a particular condition, with a function $f(x)$, where $x$ is the index integer ranging from 0 to $N-1$. The function is such that if $x$ represents the solution to the search problem $f(x)=1$, and $f(x)=0$ if $x$ is not a solution. In particular we suppose that we are supplied by a quantum \textit{oracle} $U_w$ with the ability to \textit{recognize} the solution of the search problem. The action of the oracle $U_w$ is to \textit{mark} the solution by applying a phase, namely $\ket{x}\xrightarrow{U_w}(-1)^{f(x)}\ket{x}$. \\ In a simplified manner, the quantum search algorithm proposed by Grover consists of repeated applications of a quantum subroutine known as the \textit{Grover iteration} or \textit{Grover operator} $U_G$ defined as:
\begin{equation}
  \begin{split}
    U_G & = U_s U_w \\
    & =  (2\ket{s}\bra{s}-I)U_w
  \end{split}
\end{equation}
where $I$ the identity matrix and $\ket{s}$ is the initial state given by the superposition of all the items states:
\begin{equation}
  \ket{s} = \frac{1}{\sqrt{N}}\sum_x^N\ket{x}
\end{equation}
The question that arises is what does this Grover operator do. We can show that it can be regarded as a \textit{rotation} in the 2-dimensional space spanned by the beginning vector $\ket{s}$ and the vector $\ket{w}$ representing the solution to the search problem. We can then define the normalized state $\ket{\alpha}$ and $\ket{\beta}$:
\begin{equation}
    \ket{\alpha} = \frac{1}{\sqrt{N-1}}\sum_{x\neq w}\ket{x}
\end{equation}
\vspace{-0.5cm}
\begin{equation}
  \ket{\beta} = \ket{w}
\end{equation}
representing respectively, the superposition of all states that are not a solution and the target state. Therefore the initial state $\ket{s}$ can be re-expressed as:
\begin{equation}
  \ket{s}= \sqrt{\frac{N-1}{N}}\ket{\alpha} + \sqrt{\frac{1}{N}}\ket{\beta}
\end{equation}
The effect of $U_G$ can be understood by realizing that the oracle $U_w$ performs a \textit{reflection} of $\ket{s}$ about the vector $\kalpha$ in the plane defined by $\kalpha$ and $\kbeta$. Then, $U_s$ performs another reflection about the vector $\ket{s}$. As can be seen in \Cref{fig:grover_operator}, the product of these two reflections is a rotation. By applying the Grover operator $k$ times we can rotate the initial state vector $\ket{s}$ close to $\kbeta$, that is, the solution to the search problem. The number of necessary iterations - for which the derivation is beyond the scope of this thesis - is given by $\frac{\pi}{4}\sqrt{N}$. Therefore we can identify a \textit{quantum mechanical step} with the action of the Grover operator $U_G$.
\begin{figure}[ht]
  \centering
  \includegraphics[width=70mm]{figures/chapter1/grover_reflection}
  \caption[Action of the Grover operator]{\textbf{Action of the Grover operator }$\bm{U_G}$: The figure shows the action of the Grover operator $U_G=U_s U_w$. Starting from the initial state $\ket{s}$ the action of the Oracle $U_w$ reflects it about the vector $\kalpha$. Then, the action of $U_s$ is an additional reflection about $\ket{s}$. The application of the Grover operator for $\pi/4\sqrt{N}$ times rotates the inital state bringing it close to $\kbeta$, that is, the solution of the search problem.}
  \label{fig:grover_operator}
\end{figure}


\noindent
To summarize, $U_G$ is the rotation in the 2-dimensional space spanned by $\kalpha$ and $\kbeta$. In particular, repeated applications of the Grover operator rotate the initial state vector $\ket{s}$ close to $\kbeta$. For applications of $U_G$ in the order of $O(\sqrt{N})$, a measurement of the state in the computational basis produces with high probability $\kbeta$ as the outcome, that is, the solution to the search problem.


\section{Search by quantum walks}\label{sec:search quantum walk}
The search problem first introduced by Grover can be reformulated in terms of search based on a continous-time quantum walk on a graph \cite{Childs2004}. \\In order to do so we need to modify the quantum walk Hamiltonian of \cref{qw_Hamiltonian} so that the vertex $\ket{w}$ is special. Therefore, an \textit{oracle Hamiltonian} is introduced:
\begin{equation}
  H_w = -|w\rangle\langle w|
\end{equation}
that has energy zero for all but the vertex $|w\rangle$ for which it has energy $-1$. Therefore the Grover problem becomes finding the ground state of this Hamiltonian. \\To implement the search we consider the Hamiltonian
\begin{equation}
  H = \gamma L + H_w = \gamma L -|w\rangle\langle w|
\end{equation}
where $L$ is the Laplacian of the graph $G$, that as we have seen in \Cref{sec:quantum walks} governs the evolution of the quantum walk.\\
The quantum search works as following:
\begin{itemize}
  \item we consider the balanced superposition of all possible states, namely
  \begin{equation}
    |s\rangle = \frac{1}{\sqrt{N}}\sum_j|j\rangle
  \end{equation}

  \item we run the quantum walk for a time $T$ and find the corrisponding evolved state using the Hamiltonian $H$
  \begin{equation}
  |\psi(T)\rangle = U(T)|s\rangle  = \mbox{exp}\Big\{-iHT\Big\}|s\rangle
  \end{equation}
  (Note that this evolution is valid only for time-independent Hamiltonians.)

  \item we then measure the occupation probability of the target node
  \begin{equation}
    p = |\langle w|\psi(T)\rangle|^2.
  \end{equation}

\end{itemize}
\noindent
The objective is to find the optimal value of $\gamma$ so that the success probability $|\braket{w|\psi(T)}|^2$ is as close as possible to 1 for the smallest $T$. \\
It is interesting to notice that the oracle introduced in this fashion is exactly the continous-time version of the reflection $U_w$ in Grover's algorithm \cite{Wong2016} because by itself it only evolves the marked vertex $\ket{w}$ by a phase
\begin{equation}
  e^{-i\ket{w}\bra{w}t}\ket{w} = e^{-it}\ket{w}
\end{equation}
while leaving the other vertices unchanced, thus making this the continous-time version of a yes/no oracle. \\


\noindent
We can turn our attention on trying to understand why we should expect this algorithm to give a success probability for some values of $\gamma, T$. In order to do so we frame the problem in terms of Hamiltonian spectrum \cite{Childs2004}. In particular we look at the two extremes, namely $\gamma\rightarrow\infty$ and $\gamma\rightarrow 0$.
\begin{itemize}
  \item as $\gamma\rightarrow\infty$ the contribution of $H_w$ is negligible, to the point that the ground state of $H$ is $\ket{s}$\footnote{Note that regardless of the graph topology considered, $\ket{s}$ is the ground state of the Laplacian, with eigenvalue $0$.}
  \item on the other hand, if $\gamma\rightarrow 0$ the contribution of the Laplacian to the overall Hamiltonian $H$ disappears and thus the ground state of $H$ is close to $\ket{w}$
\end{itemize}
For some intermediate $\gamma$ we have that the Hamiltonian will drive transitions between the two states, thus rotating the state from $\ket{s}$ to one with substantial overlap with $\ket{w}$. In particular this transition will happen at a time that depends on the Hamiltonian eigenvalues separation between the first and ground state, namely $1/(E_1-E_0)$.

%This is a good description of the algorithm if the dimension of the graph is sufficiently high.  However, if we consider a $d$-dimensional lattice with $d$ independent of $N$ we still see that for a critical value of $\gamma$ the state switches from the ground state to the first excited state, but for $d<4$ the $\ket{w}$ state does not have substantial overlap on the ground and fist excited state, thus the algorithm does not work. \\

\subsection{Search on the complete graph}\label{subsec:search qw complete graph}
We now look at the complete graph with $N$ nodes that represents the simplest example of the quantum walk search application \cite{Childs2004}. \\We begin by noticing that adding a multiple of the identity matrix to the Laplacian only contributes a global unobservable phase. We add $-NI$ so that:
\begin{equation}
  L - NI = N\ket{s}\bra{s}
\end{equation}
This gives us the following Hamiltonian:
\begin{equation}
  H = -\gamma N\ket{s}\bra{s} - \ket{w}\bra{w}
\end{equation}
In particular we know that this Hamiltonian acts non-trivially in a two dimensional subspace spanned by $\ket{s}$ and $\ket{w}$, thus making it stright-forward to compute its spectrum. If we use the $\{\ket{\alpha}, \ket{w}\}$ basis, the Hamiltonian can be expressed in the following way:
\begin{equation}
  H = \frac{-1}{N} \begin{pmatrix} N+1 & \sqrt{N-1}\\ \sqrt{N-1} & N-1 \end{pmatrix}
\end{equation}
where $\gamma$ is set to $1/N$.
Applying the time-evolution operator to the initial state $\ket{s}$, the system at time $t$ will be
\begin{equation}
  \ket{\psi(t)} = e^{it}
  \begin{pmatrix}
  \frac{1}{\sqrt{N}}\cos(\frac{t}{\sqrt{N}}) +i\sin(\frac{t}{\sqrt{N}})\\
  \sqrt{\frac{N-1}{N}}\cos(\frac{t}{\sqrt{N}})
  \end{pmatrix}
  \label{eq:qw_state_ref}
\end{equation}
and for $t=\pi\sqrt{N}/2$ the system reaches a success probability of 1, namely
\begin{equation}
  p = \Big|\braket{w|\psi(t)}\Big|^2_{t=\pi\sqrt{N}/2} = 1
\end{equation}
Thus the walk rotates the state from $\ket{s}$ to $\ket{w}$ in $O(\sqrt{N})$.

\section{Search by adiabatic evolution}\label{sec:adiabatic evolution}
We now address the computation by adiabatic evolution firstly introduced by Farhi et al., that takes advantage of the adiabatic theorem to find the solution of a computational problem \cite{Farhi2000}. We begin by looking at the \textit{adiabatic theorem} and its implication. Then, we overview the adiabatic implementation of the algorithm for solving the unstructured search problem which has a time scaling of $O(N)$. Lastly, we see how applying the adiabatic theorem \textit{locally} can find the solution in a time of order $O(\sqrt{N})$ which is optimal.

    \subsection{Adiabatic theorem}\label{subsec:adiabatic theorem}
    A quantum system evolves according to the Schroedinger equation
    \begin{equation}
        i\frac{d}{dt}|\psi(t)\rangle = H(t)|\psi(t)\rangle.
    \end{equation}
    The instantaneous eigenstates and eigenvalues of H(t) are given by
    \begin{equation}
        H(t)\ket{\psi_l(t)} = E_l(t)\ket{\psi_l(t)}
    \end{equation}
    such that $E_0(t) \leq E_1(t) \leq ... \leq E_{N-1}(t)$. \\
    The adiabatic theorem states that if the gap between the two lowest energy levels, $E_{1}(t) - E_{0}(t) > 0$, is stritcly greater than zero, then for $T\rightarrow \infty$ the probability of being in the ground state $\ket{\psi_0}$ is equal to one, namely
    \begin{equation}
        \lim_{T \to \infty} \braket{\psi_0(T)|\psi(T)}= 1
    \end{equation}
    This means that if the system is chosen to evolve at a slow enough rate, the instantaneous Hamiltonian will remain in the ground state throughout the evolution. Furthermore, it is convenient to parametrize the Hamiltonian as $H(s)$, where $s=t/T$, with $t \in [0,T]$ so that $s \in [0,1]$.
    Let us now define the minimum energy gap between the lowest eigenvalues of $H$ by
    \begin{equation}
        g_{min} = \min_{0 \leq s \leq 1} (E_1(s)-E_0(s))
    \end{equation}
    This allows to find a minimum time $T^*$ such that, for $T\gg T^*$ the probability of being in the ground state $\ket{\psi_0}$ is arbitrarily close to 1. In details, $T^*$ is given by:
    \begin{equation}
        T^* = \frac{\varepsilon}{g^{2}_{min}}
        \label{eq:adiabatic_time}
    \end{equation}
    where
    \begin{equation}
        \varepsilon = \max_{0 \leq s \leq 1} \Big| \Big\langle \psi_1(s)\Big| \frac{dH(s)}{ds} \Big| \psi_0(s)\Big\rangle\Big|.
    \end{equation}\\
For future convenience we will refer to the matrix element as $\braket{\frac{dH}{ds}}$, with $s\in[0,1]$. \\ \\
\noindent
Let us now discuss how to take advantage of the adiabatic theorem. Let us consider a problem Hamiltonian $H_P$ whose ground state is not so straight forward to find; on the other hand we can prepare the system in a beginning Hamiltonian $H_B$ whose ground state is known. The problem Hamiltonian encodes the solution of the problem, while the beginning Hamiltonian is a tool to easily prepare the state to be evolved. The adiabatic evolution then consists, assuming that the ground state of $H_P$ is unique, in having a time dependent Hamiltonian $H(s)$ that interpolates between $H_B$ and $H_P$
    \begin{equation}
        H(s) = (1-s)H_B + s H_P
    \end{equation}
    In this way, by tuning the value of $s$ , the Hamiltonian changes from $H_B$ to $H_P$. Therefore, if the system evolves sufficiently slowly it reaches the ground state of the problem Hamiltonian, that is exactly the solution.

    \subsection{Global adiabatic evolution}\label{subsec:global adiabatic}
    Let us now apply the adiabtic theorem to the unsorted search problem \cite{Roland2002}. As done in \Cref{sec:grover search} we consider an unsorted database of $N$ elements such that $N=2^n$, so that the elements in this particular basis can be written as $\ket{i}$, with $i=0,...,N-1$. The marked state can be then denoted as $\ket{w}$, and the initial state is the superposition of all the elements $\ket{i}$:
    \begin{equation}
      \ket{\psi_0}=\frac{1}{\sqrt{N}}\sum_{i=1}^{N-1}\ket{i}
    \end{equation}
    With this in mind we design two particular Hamiltonians $H_0$ and $H_w$ such that $\ket{\psi_0}$ is the ground state of the first while $\ket{w}$ is the ground state of the latter:
    \begin{equation}
      H_0 = I- \ket{\psi_0}\bra{\psi_0}
      \label{eq:adiabatic_ground}
    \end{equation}
    \vspace{-1cm}
    \begin{equation}
      H_w = I- \ket{w}\bra{w}
      \label{eq:adiabatic_first}
    \end{equation}
    The time-dependent Hamiltonian follows from the adiabatic implementation discussed in \Cref{sec:adiabatic evolution}, where $H_0$ is the beginning Hamiltonian and $H_w$ is the problem Hamiltonian. It thus consists in a linear interpolation between $H_0$ and $H_w$:
    \begin{equation}
      H(t) = (1-s)H_0 + sH_w
    \end{equation}
    where $s=t/T$ is the linear interpolating schedule.\\ \\
    The search routine runs as usual, with the system prepared in the state $\ket{\psi(0)}=\ket{\psi_0}$ and then we apply the Hamiltonian $H(t)$ for a time $T$. We are interested in finding the time-dependent condition such that the system evolves sufficiently slowly, allowing us to find the solution $\ket{w}$ with high probability. First, we determine the quantity $\braket{\frac{dH}{dt}}$:
    \begin{equation}
      \braket{\frac{dH(t)}{dt}} = \frac{ds}{dt}\braket{\frac{dH(s)}{ds}} = \frac{1}{T}\braket{\frac{dH(s)}{ds}}
    \end{equation}
    We then find the eigenvalues of the Hamiltonian and determine the separation $g$ between the ground state and first excited one, namely $E_1$ and $E_0$, as a function of the interpolating schedule $s$:
    \begin{equation}
      g=\sqrt{1-4\frac{N-1}{N}s(1-s)}.
      \label{separation}
    \end{equation}
    We are interested in the minimum gap $g_{\min}$ which is found for $s=1/2$. Additionally we find that $\big|\braket{\frac{dH}{ds}}_{0,1}\big| \leq 1$, therefore \Cref{eq:adiabatic_time} becomes:
    \begin{equation}
      \frac{\big|\braket{\frac{dH}{ds}}_{0,1}\big|}{g_{\min}^2} \leq \varepsilon
      \label{adiabatic_condition}
    \end{equation}
    so that the adiabatic condition is verified, and thus the Hamiltonian stays in the ground state at all times, provided that :
    \begin{equation}
      T\geq N/\varepsilon.
      \label{eq:adiabatic_scaling_global}
    \end{equation}
    Therefore the computation time is of order $N$, showing no speed up compared to the classical search.

    \subsection{Local adiabatic evolution}\label{subsec:local adiabatic}
    Roland and Cerf showed that the adiabatic evolution can be improved by applying the adiabatic condition of \cref{adiabatic_condition} locally instead of globally \cite{Roland2002}. If we look at the plot of the separation $g$ we see that the adiabatic condition is critical only for $s=1/2$ where it reaches its minimum.
    \begin{figure}[h]
      \centering
      \includegraphics[width=80mm]{figures/chapter1/separation.pdf}
      \caption[Eigenvalue separation of the time-dependent Hamiltonian $H(s)$ as a function of the reduced time $s$, for N=64]{Eigenvalue separation of the time-dependent Hamiltonian $H(s)$ as a function of the reduced time $s$, for N=64. \textit{Figure from Roland and Cerf} \cite{Roland2002}}
      \label{separation_figure}
    \end{figure}
    At the beginning and at the end of the evolution the separation is large enough so that the adiabatic condition and thus the time necessary for the system to evolve adiabatically is small, allowing for better time scaling. The improvement comes in the form of the interpolating schedule, following the idea that it should be steeper for large separation $g$ and flatter for small separation around $s=1/2$. Let us see how can this be done. \\ \\
    We divide the interval $[0,T]$ into infinitesimal intervals $dt$ and adapt the evolution rate $\frac{ds}{dt}$ to the local adabatic condition. In this way we can find the optimal $s(t)$, with boundary conditions $s(0)=0$ and $s(T)=1$. We find the new adiabatic condition for all time $t$:
    \begin{equation}
      \Big|\frac{ds}{dt}\Big|\leq\varepsilon\frac{g^2(t)}{\big|\braket{\frac{dH}{ds}}_{0,1}\big|}
    \end{equation}
    Using \cref{separation} and $\big|\braket{\frac{dH}{ds}}_{0,1}\big| \leq 1$ and Hamiltonian is chosen to evolve with the interpolating schedule that is solution of the following differential equation (with $\varepsilon<<1$):
    \begin{equation}
      \frac{ds}{dt} = \varepsilon g^2(t) = \varepsilon\Big[1-4\frac{N-1}{N}s(1-s)\Big]
    \end{equation}
    After integration we get the following
    \begin{equation}
      t =\frac{1}{2\varepsilon}\frac{N}{\sqrt{N-1}}\Big[\arctan\big(\sqrt{N-1}(2s-1)\big) + \arctan{\sqrt{N-1}}\Big]
    \end{equation}
    By inverting we find the interpolating schedule $s(t)$ as can be seen in \cref{interpolating_schedule}. In order to determine the computation time of this algorithm. To do so we evaluate $s=1$ and in the approximation that $N>>1$ we get:
    \begin{equation}
      T = \frac{\pi}{2\varepsilon}\sqrt{N}
    \end{equation}
    which represents a great improvement on \Cref{eq:adiabatic_scaling_global}. Indeed we have a quadratic speed up compared to the global adiabatic evolution, and thus this algorithm can be seen as the adiabatic implementation of the Grover's search algorithm.
    \begin{figure}[!htb]
      \centering
      \includegraphics[width=80mm]{figures/chapter1/interpolating_schedule.pdf}
      \caption[Roland and Cerf interpolating schedule for the unstructured search with N=64]{Roland and Cerf interpolating schedule for the unstructured search with N=64. \textit{Figure from Roland and Cerf} \cite{Roland2002}}
      \label{interpolating_schedule}
    \end{figure}


\clearpage
\section[Adiabatic-quantum walk search]{Impossibility of a search algoritm based on adiabatic quantum walks}\label{impossibility}
In this section we show that an \textit{adiabatic quantum walks} (AQW) based search algorithm cannot be implemented with the usual Grover's oracle and requires a more sophisticated structure. This section, based on a paper by Wong et al. \cite{Wong2016}, sets the basis on which our work builds upon. \\ \\
In the previous sections we discussed Grover's original discrete-time search algorithm, Farhi and Gutmann's quantum walk analogue and Roland and Cerf's local adiabatic analogue, where we analyzed the systems in a 2-dimensional subspace spanned by $\big\{\ket{\alpha}, \ket{w}\big\}$. \\This allows to visualize the evolution of the quantum algorithm on the two dimensional Bloch sphere and compare the path taken by each different approach. In particular we notice that the original Grover's algorithm and the Roland and Cerf's local adiabatic evolution follow the same path on the $xz$-plane since they both always have real coefficients, as can be seen in \Cref{fig:blochsphere_Grover,fig:blochsphere_RC} respectively. On the other hand the quantum walk formulation of Farhi and Gutmann has an unmeasurable complex phase thus it evolves on a different path on the $yz$-plane, see \Cref{fig:blochsphere_FG}.
\begin{figure}[h]
     \centering
     \subfloat[Grover]{\includegraphics{figures/chapter1/blochsphere_Grover}\label{fig:blochsphere_Grover}}
     \subfloat[Roland-Cerf]{\includegraphics{figures/chapter1/blochsphere_RC}\label{fig:blochsphere_RC}}
     \subfloat[Farhi-Gutmann]{\includegraphics{figures/chapter1/blochsphere_FG}\label{fig:blochsphere_FG}}
     \caption[Evolution of the quantum algorithms on the bloch sphere]{\textbf{Evolution of the quantum algorithms on the bloch sphere}. The figure shows the evolution of quantum algorithm on the Bloch sphere with marked vertex $\ket{w}$ at the North Pole and the equal superposition of the unmarked vertices $\ket{\alpha}$ at the South Pole, and $N=1024$: (a) Grover's original discrete-time search algorithm, (b) Roland and Cerf's local adiabatic evolution analogue and (c) Farhi and Gutmann's quantum walk analogue. \textit{Wong et al. (2016)} \cite{Wong2016}}
     \label{fig:blochsphere}
\end{figure}

\noindent
This substantial difference of the path taken by the different algorithms is exacly what Wong et al. investigate, in particular with the goal of determine whether it is possible to construct an adiabatic algorithm that follows the path of the quantum walks based algorithm. \\

\noindent
To do so they construct an Hamiltonian from the following states, with the first representing the ground state and the latter the first (and only) excited state -- rememeber that the Grover's search rotates the initial state over the final state, thus the first excited state is the orthogonal of the ground state.
\begin{equation}
  \ket{\psi_0(t)} = \alpha(t)\ket{w} + \beta(t)\ket{\alpha}
\end{equation}
\vspace{-0.5cm}
\begin{equation}
  \ket{\psi_1(t)} = \beta(t)\ket{w} - \alpha^*(t)\ket{\alpha}
\end{equation}
where the coefficients are given by the components of $\ket{\psi_0(t)}$ of \Cref{eq:qw_state_ref} \footnote{Note that the global phase $e^{it}$ is dropped.}.
The Hamiltonian is then given by the linear combination of $\ket{\psi_0(t)}$ and $\ket{\psi_1(t)}$:
\begin{equation}
  H(t) = \lambda_0\ket{\psi_0}\bra{\psi_0} + \lambda_1\ket{\psi_1}\bra{\psi_1}
\end{equation}
which allows to determine the final time-dependent \textit{adiabatic} Hamiltonian -- with the time-dependence given by the interpolating schedule $s(t)=\sin^2\big(\frac{t}{\sqrt{N}}\big)$:
\begin{equation}
  H(s) = \sqrt[4]{\frac{s(1-s)}{4\varepsilon^2N}}\Big[ (1-s)H_0 + sH_f + \sqrt{s(1-s)}H_e\Big].
\end{equation}
Indeed the system evolves from $t=0$ to $t=\frac{\pi}{2}\sqrt{N}$ as we would expect. Looking closely, the Hamiltonian $H_0$ represents the beginning Hamiltonian, $H_f$ the final Hamiltonian -- encoding the solution of the search problem -- and $H_e$ is an \textit{extra} Hamiltonian, a common tecnique for manupulating the evolution of the path of adiabatic algorithms\footnote{The extra Hamiltonian commonly appears as $s(s-1)$, while the one considered in this scenario is given by the square root of such value \cite{Wong2016}.}. Let us now look at the consequences of this Hamiltonian, looking at $H_0$, $H_f$ and $H_e$ individually. \\
$H_0$ has ground state $\ket{s}$ and excited state $\ket{s^\perp}$ with respective eigenvalues $-1$ and $+1$, thus can be written as:
\begin{equation}
  H_0  = \ket{s^\perp}\bra{s^\perp} - \ket{s}\bra{s}
\end{equation}
Similarly, the final Hamiltonian $H_f$ has ground state $\ket{w}$ and excited state $\ket{\alpha}$ with eigenvalues $+1$ and $-1$:
\begin{equation}
  H_f =  \ket{\alpha}\bra{\alpha} - \ket{w}\bra{w}
\end{equation}
It is immediate to notice that these two Hamiltonians look fairly similar to the initial and final Hamiltonian of the standard adiabatic quantum search algorithm of \Cref{eq:adiabatic_ground,eq:adiabatic_first}. Although the similarities look at first sight promising, the extra Hamiltonian $H_e$ and the interpolating schedule $s(t)$ change completely the evolution of the system. In particular, if we look at the Hamiltonian $H_e$ - given by:
\begin{equation}
  H_e = 2i\sqrt{\frac{N-1}{N}}\big(\ket{\alpha}\bra{w} - \ket{w}\bra{\alpha}\big)
\end{equation}
we see that rather than having the standard yes/no oracle given by $\ket{w}\bra{w}$, it introduces a more powerful structure, driving the evolution between $\ket{w}$ and $\ket{\alpha}$ instead of just applying a phase to the targer state $\ket{w}$ like in the standard Grover's algorithm. This is a confirmation that the oracle is no longer the usual Grover's yes/no oracle, and although this a proof that an adiabatic evolution following the quantum walk path does exists it abandons the usual notion of the oracle and does not solve the search problem itself as originally posed by Grover. \\

\noindent
The difference between the quantum walks and the adiabatic evolution approach is not a measure of the performance of the algorithms, since both, individually, solve the Grover's problem in $O(\sqrt{N})$ time, but illustrate that ``\textit{how} they compute is different, even though \textit{what} they compute is the same '' \cite{Wong2016}.\\

\noindent
Although the work by Wong et al. shows that is not possible to implement an adiabatic quantum walk algorithm that solves the Grover's search problem, it leaves space to a time-dependent algorithm \textit{inspired} by the adiabatic evolution, but that it is free from the constrains of the adiabatic condition. In the next chapter we investigate this possibility, that gives some interesting insights and properties of the quantum walks search algorithm with time-dependent Hamiltonians.
